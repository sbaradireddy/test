

To find out which specific POL_PK values have NULL values associated with them in your SQL query, you can focus on capturing those POL_PK values that result in a NULL value for ORIG_POL_PK.

Here’s how you can structure your SQL query to identify these specific POL_PK values:

SQL Query to Identify Specific POL_PK with NULL Values

WITH SQ_S10_MA_DMV_EXTRACT AS (
    SELECT T_POL_DIM_HST.POL_PK
    FROM CDWQA.dwadmn.v_pol_dim_hst_core T_POL_DIM_HST,
         CDWQA.dwadmn.t_pol_pty_dim_hst T_POL_PTY_DIM_HST,
         CDWQA.dwadmn.v_pol_veh_unit_dim_hst_core T_POL_VEH_UNIT_DIM_HST,
         CDWQA.dwadmn.t_co_dim T_CO_DIM,
         CDWQA.dwadmn.t_pol_addr_dim_hst T_POL_ADDR_DIM_HST,
         CDWQA.dwadmn.t_pol_stat_trans_hst T_POL_STAT_TRANS_HST,
         CDWQA.dwadmn.t_pol_cvg_fact T_POL_CVG_FACT 
    WHERE T_POL_DIM_HST.CO_CD IN ('ALN_PRAC', 'ALN_PIC', 'PMC', 'PRAC')
      AND (
          (T_POL_DIM_HST.PROD_CD IN ('PA') 
           AND T_POL_VEH_UNIT_DIM_HST.VEH_TYP_CD IN ('AQ', 'CL', 'MC', 'MH', 'PPA'))
          OR
          (T_POL_DIM_HST.PROD_CD IN ('CA') 
           AND T_POL_VEH_UNIT_DIM_HST.VEH_TYP_CD IN ('PP', 'STV', 'TR', 'TR_TRAC', 'VAN'))
      )
      AND T_POL_DIM_HST.STATE_CD = 'CT'  
      AND T_POL_DIM_HST.RCD_ACTN_TYP <> 'D' 
      AND (
          T_POL_STAT_TRANS_HST.POL_STAT_PK IN (
              SELECT MAX(AL5.POL_STAT_PK)
              FROM CDWQA.DWADMN.V_POL_STAT_TRANS_HST AL5
              WHERE AL5.DATA_DT <= '2024-09-01' 
                AND AL5.POL_PK = T_POL_STAT_TRANS_HST.POL_PK
          )
          AND T_POL_STAT_TRANS_HST.POL_STAT = 'ACTIVE'
          AND T_POL_STAT_TRANS_HST.POL_STATE_EFF_DT <= '2024-09-01'
          AND T_POL_STAT_TRANS_HST.ROW_XPTN_DT > '2024-09-01' 
          AND T_POL_CVG_FACT.CNTRBT_TO_PRMM_FLG = 'Y'
          AND T_POL_CVG_FACT.RCD_ACTN_TYP <> 'D'
      )
      AND T_POL_ADDR_DIM_HST.PMRY_ADDR_FLG = 'Y'
      AND T_POL_PTY_DIM_HST.NAMD_INSD_FLG = 'Y' 
      AND T_POL_VEH_UNIT_DIM_HST.POL_STATE_STAT = 'ACTIVE'  
      AND T_POL_VEH_UNIT_DIM_HST.RCD_ACTN_TYP <> 'D' 
      AND T_POL_DIM_HST.POL_PK = T_POL_PTY_DIM_HST.POL_PK 
      AND T_POL_VEH_UNIT_DIM_HST.POL_PK = T_POL_DIM_HST.POL_PK 
      AND T_CO_DIM.CO_ID = T_POL_DIM_HST.CO_ID
      AND T_POL_DIM_HST.POL_PK = T_POL_ADDR_DIM_HST.POL_PK 
      AND T_POL_STAT_TRANS_HST.POL_PK = T_POL_DIM_HST.POL_PK 
      AND T_POL_CVG_FACT.POL_PK = T_POL_DIM_HST.POL_PK
), 
SQ_S10_MA_DMV_EXTRACT_TEMP AS (
    -- Query for source qualifier function
    SELECT row_number() OVER (ORDER BY POL_PK) AS source_record_id,
           POL_PK
    FROM SQ_S10_MA_DMV_EXTRACT
), 
EXPTRANS AS (
    -- Query for expression function
    SELECT SQ_S10_MA_DMV_EXTRACT_TEMP.source_record_id,
           SQ_S10_MA_DMV_EXTRACT_TEMP.POL_PK,
           LKP_PRIOR_VERSN_POL_PK.ORIG_POL_PK
    FROM SQ_S10_MA_DMV_EXTRACT_TEMP
    LEFT JOIN (
        -- Query for lookup function
        SELECT ORIG_POL_PK,
               POL_PK,
               row_number() OVER (PARTITION BY POL_PK ORDER BY POL_PK) AS rn
        FROM CDWQA.DWADMN.T_POL_STAT_DIM_HST
        WHERE LTRIM(RTRIM(CDWQA.DWADMN.T_POL_STAT_DIM_HST.STAT_TRAN_CODE)) = 'CREDIT'
        QUALIFY rn = 1
    ) LKP_PRIOR_VERSN_POL_PK
    ON LKP_PRIOR_VERSN_POL_PK.POL_PK = SQ_S10_MA_DMV_EXTRACT_TEMP.POL_PK
)

-- Final SELECT statement to retrieve POL_PK with NULL ORIG_POL_PK
SELECT SQ_S10_MA_DMV_EXTRACT_TEMP.POL_PK,
       SQ_S10_MA_DMV_EXTRACT_TEMP.source_record_id
FROM SQ_S10_MA_DMV_EXTRACT_TEMP
LEFT JOIN EXPTRANS ON SQ_S10_MA_DMV_EXTRACT_TEMP.POL_PK = EXPTRANS.POL_PK
WHERE EXPTRANS.ORIG_POL_PK IS NULL;  -- Check for NULL values in ORIG_POL_PK

Explanation of Key Sections:

	1.	CTEs:
	•	SQ_S10_MA_DMV_EXTRACT: Extracts the relevant POL_PK values based on various filtering criteria.
	•	SQ_S10_MA_DMV_EXTRACT_TEMP: Adds a sequential source_record_id to each POL_PK.
	•	EXPTRANS: Joins the temporary extract with the lookup to get ORIG_POL_PK.
	2.	Final Selection:
	•	The final SELECT statement retrieves the POL_PK and source_record_id from the SQ_S10_MA_DMV_EXTRACT_TEMP where the associated ORIG_POL_PK in EXPTRANS is NULL. This allows you to see which specific POL_PK values have NULL for ORIG_POL_PK.

Result:

By executing this query, you’ll get a list of POL_PK values along with their source_record_id that are associated with NULL values in ORIG_POL_PK. This helps you pinpoint the exact entries that need attention for further data validation or correction.

import boto3
import base64
import os
import requests
import uuid
import smtplib
from email.mime.text import MIMEText
from datetime import datetime, timedelta
from msal import ConfidentialClientApplication

def lambda_handler(event, context):
    scopes = ['https://graph.microsoft.com/.default']

    # Azure AD App details
    CLIENT_ID = 'kg,jkhg'
    CLIENT_SECRET = 'mhgv,jhgv'
    TENANT_ID = 'jhfjhcfv'
	
    # AWS S3 bucket details
    S3_BUCKET = "pr-home-datascience/email_upload"
    
    # SMTP configuration
    SMTP_SERVER = "smtp.example.com"
    SMTP_PORT = 587
    SMTP_USERNAME = "your-email@example.com"
    SMTP_PASSWORD = "your-email-password"
    RECIPIENT_EMAIL = "recipient@example.com"

    # Set up MSAL
    authority = f'https://login.microsoftonline.com/{TENANT_ID}'
    app = ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)

    # Authenticate and get token
    result = app.acquire_token_for_client(scopes=scopes)
    access_token = result['access_token']

    # Calculate time range (last 2 hours)
    now = datetime.utcnow()
    two_hours_ago = now - timedelta(hours=2)

    # Convert to ISO 8601 format for Microsoft Graph
    start_time = two_hours_ago.isoformat() + "Z"  # 'Z' for UTC
    end_time = now.isoformat() + "Z"

    # Microsoft Graph API URL to fetch emails from the last 2 hours
    graph_url = f'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/mailFolders/inbox/messages?$filter=receivedDateTime ge {start_time} and receivedDateTime le {end_time}'

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Accept': 'application/json'
    }
    
    s3 = boto3.client('s3')

    def fetch_emails(graph_url):
        while graph_url:
            response = requests.get(graph_url, headers=headers)
            emails = response.json().get('value', [])
            process_emails(emails)
            
            # Check if there is another page of results
            graph_url = response.json().get('@odata.nextLink')
            print(graph_url)

    def process_emails(emails):
        print('emails count - ', len(emails))
        for email in emails:
            # Generate a unique UUID for folder name
            folder_name = str(uuid.uuid4()) + '/'
            
            # Check if email with the same id (or UUID) already exists in S3
            if email_already_exists(email['id'], folder_name):
                print(f"Email {email['id']} already exists in S3. Skipping.")
                continue
            
            try:
                # Create a folder in S3 if it doesn't exist
                s3.put_object(Bucket=S3_BUCKET, Key=folder_name+'/', Body=b'')
                print(f"Folder {folder_name} created in Bucket")
                
                email_content = email['subject'] + email['body']['content']
                
                # Create a txt file with email subject and body in /tmp
                filename = '/tmp/body.HTML'
                with open(filename, 'w', encoding='utf-8') as file:
                    file.write(email_content)
                    
                s3_key = f"{folder_name}/{os.path.basename(filename)}"
                s3.upload_file(filename, S3_BUCKET, s3_key)
                print(f"Uploaded email content as {filename} to S3")
        
            except Exception as e:
                print('An error occurred', e)
        
            # Fetch attachments
            attachments_url = f'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/messages/{email["id"]}/attachments'
            response1 = requests.get(attachments_url, headers=headers)
            attachments = response1.json().get('value', [])
            
            for attachment in attachments:
                if '@odata.type' in attachment and attachment['@odata.type'] == '#microsoft.graph.fileAttachment':
                    attachment_content = base64.b64decode(attachment['contentBytes'])
                    attachment_name = '/tmp/' + attachment['name']
                    
                    # Save attachment in /tmp directory
                    with open(attachment_name, 'wb') as f:
                        f.write(attachment_content)
    
                    # Upload to S3
                    with open(attachment_name, 'rb') as f:
                        s3.upload_fileobj(f, S3_BUCKET, f"{folder_name}/{os.path.basename(attachment_name)}")
                    print(f"Uploaded {attachment_name} to S3")
            
            # Store the email ID in S3 to avoid reprocessing
            store_email_id(email['id'], folder_name)

            # Send an email notification after files are uploaded to S3
            send_notification_email(folder_name)

    def email_already_exists(email_id, folder_name):
        """
        Check if the email ID (or folder) already exists in S3 to avoid duplicates.
        """
        try:
            response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=f"{folder_name}")
            # If folder already exists or UUID is already in S3, skip processing
            if 'Contents' in response:
                return True
        except Exception as e:
            print(f"Error checking S3 for existing email: {e}")
        return False

    def store_email_id(email_id, folder_name):
        """
        Store a marker in S3 to indicate the email has been processed.
        """
        try:
            marker_file = f"{folder_name}/processed_email.txt"
            s3.put_object(Bucket=S3_BUCKET, Key=marker_file, Body=email_id.encode())
            print(f"Stored processed email ID {email_id} in S3 at {marker_file}")
        except Exception as e:
            print(f"Error storing email ID in S3: {e}")

    def send_notification_email(folder_name):
        try:
            subject = "S3 Upload Notification"
            body = f"Files have been uploaded to the following S3 folder: {folder_name}"
            msg = MIMEText(body)
            msg["Subject"] = subject
            msg["From"] = SMTP_USERNAME
            msg["To"] = RECIPIENT_EMAIL

            with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
                server.starttls()
                server.login(SMTP_USERNAME, SMTP_PASSWORD)
                server.sendmail(SMTP_USERNAME, RECIPIENT_EMAIL, msg.as_string())
                print(f"Notification email sent to {RECIPIENT_EMAIL}")
        except Exception as e:
            print(f"Error sending email: {e}")

    # Start fetching emails
    fetch_emails(graph_url)





















import boto3
import base64
import os
import requests
import uuid
import smtplib
from email.mime.text import MIMEText
from msal import ConfidentialClientApplication

def lambda_handler(event, context):
    scopes = ['https://graph.microsoft.com/.default']

    # Azure AD App details
    CLIENT_ID = 'kg,jkhg'
    CLIENT_SECRET = 'mhgv,jhgv'
    TENANT_ID = 'jhfjhcfv'
	
    # AWS S3 bucket details
    S3_BUCKET = "pr-home-datascience/email_upload"
    
    # SMTP configuration
    SMTP_SERVER = "smtp.example.com"
    SMTP_PORT = 587
    SMTP_USERNAME = "your-email@example.com"
    SMTP_PASSWORD = "your-email-password"
    RECIPIENT_EMAIL = "recipient@example.com"

    # Set up MSAL
    authority = f'https://login.microsoftonline.com/{TENANT_ID}'
    app = ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)

    # Authenticate and get token
    result = app.acquire_token_for_client(scopes=scopes)
    access_token = result['access_token']

    graph_url = 'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/mailFolders/inbox/messages'

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Accept': 'application/json'
    }
    
    s3 = boto3.client('s3')

    def fetch_emails(graph_url):
        while graph_url:
            response = requests.get(graph_url, headers=headers)
            emails = response.json().get('value', [])
            process_emails(emails)
            
            # Check if there is another page of results
            graph_url = response.json().get('@odata.nextLink')
            print(graph_url)

    def process_emails(emails):
        print('emails count - ', len(emails))
        for email in emails:
            # Generate a unique UUID for folder name
            folder_name = str(uuid.uuid4()) + '/'
            
            try:
                response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=folder_name)
                if 'Contents' in response or 'CommonPrefixes' in response:
                    print('Email already exists in bucket')
                    continue
                else:
                    s3.put_object(Bucket=S3_BUCKET, Key=folder_name+'/', Body=b'')
                    print(f"Folder {folder_name} created in Bucket")
                    
                    email_content = email['subject'] + email['body']['content']
                    
                    # Create a txt file with email subject and body in /tmp
                    filename = '/tmp/body.HTML'
                    with open(filename, 'w', encoding='utf-8') as file:
                        file.write(email_content)
                        
                    s3_key = f"{folder_name}/{os.path.basename(filename)}"
                    s3.upload_file(filename, S3_BUCKET, s3_key)
                    print(f"Uploaded email content as {filename} to S3")
            
            except Exception as e:
                print('An error occurred', e)
            
            # Fetch attachments
            attachments_url = f'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/messages/{email["id"]}/attachments'
            response1 = requests.get(attachments_url, headers=headers)
            attachments = response1.json().get('value', [])
            
            for attachment in attachments:
                if '@odata.type' in attachment and attachment['@odata.type'] == '#microsoft.graph.fileAttachment':
                    attachment_content = base64.b64decode(attachment['contentBytes'])
                    attachment_name = '/tmp/' + attachment['name']
                    
                    # Save attachment in /tmp directory
                    with open(attachment_name, 'wb') as f:
                        f.write(attachment_content)
    
                    # Upload to S3
                    with open(attachment_name, 'rb') as f:
                        s3.upload_fileobj(f, S3_BUCKET, f"{folder_name}/{os.path.basename(attachment_name)}")
                    print(f"Uploaded {attachment_name} to S3")
            
            # Send an email notification after files are uploaded to S3
            send_notification_email(folder_name)

    def send_notification_email(folder_name):
        try:
            subject = "S3 Upload Notification"
            body = f"Files have been uploaded to the following S3 folder: {folder_name}"
            msg = MIMEText(body)
            msg["Subject"] = subject
            msg["From"] = SMTP_USERNAME
            msg["To"] = RECIPIENT_EMAIL

            with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
                server.starttls()
                server.login(SMTP_USERNAME, SMTP_PASSWORD)
                server.sendmail(SMTP_USERNAME, RECIPIENT_EMAIL, msg.as_string())
                print(f"Notification email sent to {RECIPIENT_EMAIL}")
        except Exception as e:
            print(f"Error sending email: {e}")

    # Start fetching emails
    fetch_emails(graph_url)













import msal
import requests
import boto3
import base64
import os
#from datetime import datetime

def lambda_handler(event, context):
    scopes = ['https://graph.microsoft.com/.default']

    # Azure AD App details
    CLIENT_ID = 'kg,jkhg'
    CLIENT_SECRET = mhgv,jhgv'
    TENANT_ID = 'jhfjhcfv'
	
    # AWS S3 bucket details
    S3_BUCKET = "pr-home-datascience/email_upload"

    # Set up MSAL
    authority = f'https://login.microsoftonline.com/{TENANT_ID}'
    app = msal.ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)

    # Authenticate and get token
    result = app.acquire_token_for_client(scopes=scopes)
    access_token = result['access_token']

    graph_url = 'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/mailFolders/inbox/messages'

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Accept': 'application/json'
    }
    
    s3 = boto3.client('s3')
    
    def fetch_emails(graph_url):
        while graph_url:
            response = requests.get(graph_url, headers=headers)
            emails = response.json().get('value', [])
            process_emails(emails)
            
            # Check if there is another page of results
            graph_url = response.json().get('@odata.nextLink')
            print(graph_url)
            
    def process_emails(emails):
        # response = requests.get(graph_url, headers=headers)
        # emails_with_attachments = response.json().get('value', [])
        print('emails count - ',len(emails))
        for email in emails:
            email_id = email['id']
            split_email_id = email_id.split('-')
            if len(split_email_id) > 2:
                folder_name = split_email_id[2].rstrip('/') + '/'
            else:
                folder_name = email_id
            #folder_name=str(datetime.now())
            try:
                response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=folder_name)
                if 'Contents' in response or 'CommonPrefixes' in response:
                    print('Email already exists in bucket')
                    continue
                else:
                    s3.put_object(Bucket=S3_BUCKET, Key=folder_name+'/',Body=b'')
                    print(f"Folder {folder_name} created in Bucket")
                    
                    email_content = email['subject'] + email['body']['content']
                    
                    # Create a txt file with email subject and body in /tmp
                    filename = '/tmp/body.HTML'
                    with open(filename, 'w', encoding='utf-8') as file:
                        file.write(email_content)
                        
                    s3_key = f"{folder_name}/{os.path.basename(filename)}"
                    s3.upload_file(filename, S3_BUCKET, s3_key)
                    print(f"Uploaded email content as {filename} to S3")
            
            except Exception as e:
                print('An error occurred', e)
            
            # Fetch attachments
            attachments_url = f'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/messages/{email_id}/attachments'
            response1 = requests.get(attachments_url, headers=headers)
            attachments = response1.json().get('value', [])
            
            for attachment in attachments:
                if '@odata.type' in attachment and attachment['@odata.type'] == '#microsoft.graph.fileAttachment':
                    attachment_content = base64.b64decode(attachment['contentBytes'])
                    attachment_name = '/tmp/' + attachment['name']  # Use /tmp for Lambda's writable space
                    
                    # Save attachment in /tmp directory
                    with open(attachment_name, 'wb') as f:
                        f.write(attachment_content)
    
                    # Upload to S3
                    with open(attachment_name, 'rb') as f:
                        s3.upload_fileobj(f, S3_BUCKET, f"{folder_name}/{os.path.basename(attachment_name)}")
                    print(f"Uploaded {attachment_name} to S3")
            response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=folder_name+'/')
            for obj in response['Contents']:
                print(obj['Key'])
                # s3.delete_object(Bucket=S3_BUCKET, Key=obj['Key'])
    
    # Start fetching emails
    fetch_emails(graph_url)
